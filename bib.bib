
@inproceedings{chernov_survey_2018,
	title = {Survey on {Deduplication} {Techniques} in {Flash}-{Based} {Storage}},
	volume = {426},
	doi = {10.23919/FRUCT.2018.8468295},
	booktitle = {Proceedings of the {XXth} {Conference} of {Open} {Innovations} {Association} {FRUCT}},
	author = {Chernov, Ilva and Ivashko, Evgenv and Rumiantsev, Alexander and Ponomarev, Vadim and Shabaev, Anton},
	month = may,
	year = {2018},
	pages = {25--33},
}

@inproceedings{zhan_how_2020,
	address = {USA},
	series = {{FAST}'20},
	title = {How to {Copy} {Files}},
	isbn = {978-1-939133-12-0},
	abstract = {Making logical copies, or clones, of files and directories is critical to many real-world applications and workflows, including backups, virtual machines, and containers. An ideal clone implementation meets the following performance goals: (1) creating the clone has low latency; (2) reads are fast in all versions (i.e., spatial locality is always maintained, even after modifications); (3) writes are fast in all versions; (4) the overall system is space efficient. Implementing a clone operation that realizes all four properties, which we call a nimble clone, is a long-standing open problem.This paper describes nimble clones in BetrFS, an opensource, full-path-indexed, and write-optimized file system. The key observation behind our work is that standard copy-on-write heuristics can be too coarse to be space efficient, or too fine-grained to preserve locality. On the other hand, a write-optimized key-value store, as used in BetrFS or an LSM-tree, can decouple the logical application of updates from the granularity at which data is physically copied. In our write-optimized clone implementation, data sharing among clones is only broken when a clone has changed enough to warrant making a copy, a policy we call copy-on-abundant-write.We demonstrate that the algorithmic work needed to batch and amortize the cost of BetrFS clone operations does not erode the performance advantages of baseline BetrFS; BetrFS performance even improves in a few cases. BetrFS cloning is efficient; for example, when using the clone operation for container creation, BetrFS outperforms a simple recursive copy by up to two orders-of-magnitude and outperforms file systems that have specialized LXC backends by 3-4×.},
	booktitle = {Proceedings of the 18th {USENIX} {Conference} on {File} and {Storage} {Technologies}},
	publisher = {USENIX Association},
	author = {Zhan, Yang and Conway, Alex and Jiao, Yizheng and Mukherjee, Nirjhar and Groombridge, Ian and Bender, Michael A. and Farach-Colton, Martín and Jannen, William and Johnson, Rob and Porter, Donald E. and Yuan, Jun},
	year = {2020},
	note = {event-place: Santa Clara, CA, USA},
	pages = {75--90},
}

@article{meyer_study_2012,
	title = {A {Study} of {Practical} {Deduplication}},
	volume = {7},
	issn = {1553-3077},
	url = {https://doi.org/10.1145/2078861.2078864},
	doi = {10.1145/2078861.2078864},
	abstract = {We collected file system content data from 857 desktop computers at Microsoft over a span of 4 weeks. We analyzed the data to determine the relative efficacy of data deduplication, particularly considering whole-file versus block-level elimination of redundancy. We found that whole-file deduplication achieves about three quarters of the space savings of the most aggressive block-level deduplication for storage of live file systems, and 87\% of the savings for backup images. We also studied file fragmentation, finding that it is not prevalent, and updated prior file system metadata studies, finding that the distribution of file sizes continues to skew toward very large unstructured files.},
	number = {4},
	journal = {ACM Trans. Storage},
	author = {Meyer, Dutch T. and Bolosky, William J.},
	month = feb,
	year = {2012},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {data, Deduplication, filesystem, study, Windows},
}

@inproceedings{policroniades_alternatives_2004,
	address = {USA},
	series = {{ATEC} '04},
	title = {Alternatives for {Detecting} {Redundancy} in {Storage} {Systems} {Data}},
	abstract = {Storage systems frequently maintain identical copies of data. Identifying such data can assist in the design of solutions in which data storage, transmission, and management are optimised. In this paper we evaluate three methods used to discover identical portions of data: whole file content hashing, fixed size blocking, and a chunking strategy that uses Rabin fingerprints to delimit content-defined data chunks. We assess how effective each of these strategies is in finding identical sections of data. In our experiments, we analysed diverse data sets from a variety of different types of storage systems including a mirrored section of sunsite.org.uk, different data profiles in the file system infrastructure of the Cambridge University Computer Laboratory, source code distribution trees, compressed data, and packed files. We report our experimental results and present a comparative analysis of these techniques. This study also shows how levels of similarity differ between data sets and file types. Finally, we discuss the advantages and disadvantages in the application of these methods in the light of our experimental results.},
	booktitle = {Proceedings of the {Annual} {Conference} on {USENIX} {Annual} {Technical} {Conference}},
	publisher = {USENIX Association},
	author = {Policroniades, Calicrates and Pratt, Ian},
	year = {2004},
	note = {event-place: Boston, MA},
	pages = {6},
}

@incollection{thwel_classification_2021,
	title = {Classification criteria for data deduplication methods},
	isbn = {978-0-12-823395-5},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128233955000112},
	abstract = {Data deduplication refers to size reduction of data by eliminating data redundancy due to duplication. Possibility of duplication is high when size of data is huge. As the data especially digital data is growing drastically on the Internet due to emerging online ways of communication and interaction in various areas such as social media, banking, and marketing, the problem of duplicate data has become serious. There are various data deduplication techniques that can be used to reduce its size. Apart from reducing the required storage space this reduction may result into different adjoining benefits. For example, it saves device cost and time required for backup and archive when data is to be stored on secondary storage. In case of primary storage, it eliminates duplicate disk I/Os and thus reduce the time of program execution. When data is meant for cloud storage, deduplication reduces time for data uploading on WAN. When data is to be stored on virtual machine, it saves time for its migration. When data is on network, its size reduction reduces time of transmission and reduces redundancy for WAN optimization. Different techniques for data deduplication may be categorized based on various aspects of algorithm design and scenarios it is designed for such as location, time, unit of deduplication, and so on. Based on location, they can be categorized as source or client depending on whether this operation is performed on client side or server side during communication of data. Whereas, based on time it depends on whether the process is performed while the data was being stored or after the data was stored. The categorization based on unit of deduplication is popularly done as file chunking, fixed block chunking and variable block chunking. Data deduplication consists of primarily identification of the duplicate data elements and replacing them by links such that all such duplicate data elements may be located physically at single position on physical media. Various steps involved in the process of data deduplication are chunking and fingerprinting, hashing, indexing, and compression.},
	booktitle = {Data {Deduplication} {Approaches}},
	publisher = {Academic Press},
	author = {Bansal, Sulabh and Sharma, Prakash Chandra},
	editor = {Thwel, Tin Thein and Sinha, G. R.},
	year = {2021},
	doi = {https://doi.org/10.1016/B978-0-12-823395-5.00011-2},
	keywords = {classification, Data deduplication, redundancy elimination},
	pages = {69--96},
}

@book{noauthor_ioctl_ficlone2_2013,
	edition = {4.3},
	title = {ioctl\_ficlone(2) {Linux} {User}'s {Manual}},
	month = jul,
	year = {2013},
}

@article{appaji_nag_yasa_space_2012,
	title = {Space {Savings} and {Design} {Considerations} in {Variable} {Length} {Deduplication}},
	volume = {46},
	issn = {0163-5980},
	url = {https://doi.org/10.1145/2421648.2421657},
	doi = {10.1145/2421648.2421657},
	abstract = {Explosion of data growth and duplication of data in enterprises has led to the deployment of a variety of deduplication technologies. However not all deduplication technologies serve the needs of every workload. Most prior research in deduplication concentrates on fixed block size (or variable block size at a fixed block boundary) deduplication which provides sub-optimal space efficiency in workloads where the duplicate data is not block aligned. Workloads also differ in the nature of operations and their priorities thereby affecting the choice of the right flavor of deduplication. Object workloads for instance, hold multiple versions of archived documents that have a high degree of duplicate data. They are also write-once read-many in nature and follow a whole object GET, PUT and DELETE model and would be better served by a deduplication strategy that takes care of nonblock aligned changes to data.In this paper, we describe and evaluate a hybrid of a variable length and block based deduplication that is hierarchical in nature. We are motivated by the following insights from real world data: (a) object workload applications do not do in-place modification of data and hence new versions of objects are written again as a whole (b) significant amount of data among different versions of the same object is shareable but the changes are usually not block aligned. While the second point is the basis for variable length technique, both the above insights motivate our hierarchical deduplication strategy.We show through experiments with production data-sets from enterprise environments that this provides up to twice the space savings compared to a fixed block deduplication.},
	number = {3},
	journal = {SIGOPS Oper. Syst. Rev.},
	author = {Appaji Nag Yasa, Giridhar and Nagesh, P. C.},
	month = dec,
	year = {2012},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {deduplication, space efficiency},
	pages = {57--64},
}

@article{zhan_how_2020-1,
	title = {How to {Not} {Copy} {Files}},
	journal = {USENIX PATRONS},
	author = {Zhan, Yang and Conway, Alex and Mukherjee, Nirjhar and Groombridge, Ian and Farach-Colton, Martin and Johnson, Rob and Jiao, Yizheng and Bender, Michael A and Jannen, William and Porter, Donald E and {others}},
	year = {2020},
	pages = {12},
}

@article{rodeh_btrfs_2013,
	title = {{BTRFS}: {The} {Linux} {B}-{Tree} {Filesystem}},
	volume = {9},
	issn = {1553-3077},
	url = {https://doi.org/10.1145/2501620.2501623},
	doi = {10.1145/2501620.2501623},
	abstract = {BTRFS is a Linux filesystem that has been adopted as the default filesystem in some popular versions of Linux. It is based on copy-on-write, allowing for efficient snapshots and clones. It uses B-trees as its main on-disk data structure. The design goal is to work well for many use cases and workloads. To this end, much effort has been directed to maintaining even performance as the filesystem ages, rather than trying to support a particular narrow benchmark use-case.Linux filesystems are installed on smartphones as well as enterprise servers. This entails challenges on many different fronts.—Scalability. The filesystem must scale in many dimensions: disk space, memory, and CPUs.—Data integrity. Losing data is not an option, and much effort is expended to safeguard the content. This includes checksums, metadata duplication, and RAID support built into the filesystem.—Disk diversity. The system should work well with SSDs and hard disks. It is also expected to be able to use an array of different sized disks, which poses challenges to the RAID and striping mechanisms.This article describes the core ideas, data structures, and algorithms of this filesystem. It sheds light on the challenges posed by defragmentation in the presence of snapshots, and the tradeoffs required to maintain even performance in the face of a wide spectrum of workloads.},
	number = {3},
	journal = {ACM Trans. Storage},
	author = {Rodeh, Ohad and Bacik, Josef and Mason, Chris},
	month = aug,
	year = {2013},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {filesystem, B-trees, concurrency, copy-on-write, RAID, shadowing, snapshots},
}

@inproceedings{zhou_characterizing_2013,
	title = {Characterizing the efficiency of data deduplication for big data storage management},
	doi = {10.1109/IISWC.2013.6704674},
	booktitle = {2013 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	author = {Zhou, Ruijin and Liu, Ming and Li, Tao},
	year = {2013},
	pages = {98--108},
}

@book{wojslaw_introducing_2017,
	title = {Introducing {ZFS} on {Linux}: {Understand} the {Basics} of {Storage} with {ZFS}},
	isbn = {978-1-4842-3305-4},
	shorttitle = {Introducing {ZFS} on {Linux}},
	abstract = {Learn the basics of do-it-yourself ZFS storage on Linux. This book delivers explanations of key features and provides best practices for planning, creating and sharing your storage.
ZFS as a file system simplifies many aspects of the storage administrator's day-to-day job and solves a lot of problems that administrators face, but it can be confusing. Introducing ZFS on Linux addresses some of these issues and shows you how to resolve them.
This book explains the technical side of ZFS, through planning the hardware list to planning the physical and logical layout of the storage.},
	publisher = {Apress},
	author = {Wojsław, Damian},
	month = jan,
	year = {2017},
	doi = {10.1007/978-1-4842-3306-1},
	file = {Full Text:/Users/kewing/Zotero/storage/I5IE46AK/Wojsław - 2017 - Introducing ZFS on Linux Understand the Basics of.pdf:application/pdf},
}

@article{zhan_copy--abundant-write_2021,
	title = {Copy-on-{Abundant}-{Write} for {Nimble} {File} {System} {Clones}},
	volume = {17},
	issn = {1553-3077},
	url = {https://doi.org/10.1145/3423495},
	doi = {10.1145/3423495},
	abstract = {Making logical copies, or clones, of files and directories is critical to many real-world applications and workflows, including backups, virtual machines, and containers. An ideal clone implementation meets the following performance goals: (1) creating the clone has low latency; (2) reads are fast in all versions (i.e., spatial locality is always maintained, even after modifications); (3) writes are fast in all versions; (4) the overall system is space efficient. Implementing a clone operation that realizes all four properties, which we call a nimble clone, is a long-standing open problem.This article describes nimble clones in B-ϵ-tree File System (BetrFS), an open-source, full-path-indexed, and write-optimized file system. The key observation behind our work is that standard copy-on-write heuristics can be too coarse to be space efficient, or too fine-grained to preserve locality. On the other hand, a write-optimized key-value store, such as a Bε-tree or an log-structured merge-tree (LSM)-tree, can decouple the logical application of updates from the granularity at which data is physically copied. In our write-optimized clone implementation, data sharing among clones is only broken when a clone has changed enough to warrant making a copy, a policy we call copy-on-abundant-write.We demonstrate that the algorithmic work needed to batch and amortize the cost of BetrFS clone operations does not erode the performance advantages of baseline BetrFS; BetrFS performance even improves in a few cases. BetrFS cloning is efficient; for example, when using the clone operation for container creation, BetrFS outperforms a simple recursive copy by up to two orders-of-magnitude and outperforms file systems that have specialized Linux Containers (LXC) backends by 3–4×.},
	number = {1},
	journal = {ACM Trans. Storage},
	author = {Zhan, Yang and Conway, Alex and Jiao, Yizheng and Mukherjee, Nirjhar and Groombridge, Ian and Bender, Michael A. and Farach-Colton, Martin and Jannen, William and Johnson, Rob and Porter, Donald E. and Yuan, Jun},
	month = jan,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Bε-trees, clone, file system, write optimization},
}

@inproceedings{bolosky_single_2000,
	title = {Single {Instance} {Storage} in {Windows} 2000},
	url = {https://www.microsoft.com/en-us/research/publication/single-instance-storage-in-windows-2000/},
	abstract = {Certain applications, such as Windows 2000's Remote Install service, can result in a set of files in which many different files have the same content. Using a traditional file system to store these files separately results in excessive use of disk and main memory file cache space. Using hard or symbolic links would eliminate the excess resource requirements, but changes the semantics of having separate files, in that updates to one "copy" of a file would be visible to users of another "copy." We describe the Single Instance Store (SIS), a component within Windows© 2000 that implements links with the semantics of copies for files stored on a Windows 2000 NTFS volume. SIS uses copy-on-close to implement the copy semantics of its links. SIS is structured as a file system filter driver that implements links and a user level service that detects duplicate files and reports them to the filter for conversion into links. Because SIS links are semantically identical to separate files, SIS creates them automatically when it detects files with duplicate contents. This paper describes the design and implementation of SIS in detail, briefly presents measurements of a remote install server showing a 58\% disk space savings by using SIS, and discusses other possible uses of SIS.},
	booktitle = {Proceedings of 4th {USENIX} {Windows} {Systems} {Symposium}},
	publisher = {USENIX},
	author = {Bolosky, Bill and Corbin, Scott and Goebel, David and Douceur, John (JD)},
	month = jan,
	year = {2000},
	note = {Edition: Proceedings of 4th USENIX Windows Systems Symposium},
}

@patent{williams_method_1999,
	title = {Method for partitioning a block of data into subblocks and for storing and communcating such subblocks},
	url = {https://patents.google.com/patent/US5990810/en},
	nationality = {US},
	assignee = {Trustus Pty Ltd},
	number = {US5990810A},
	urldate = {2023-01-20},
	author = {Williams, Ross Neil},
	month = nov,
	year = {1999},
	keywords = {block, partitioning, subblock, subblocks, zero},
	file = {Full Text PDF:/Users/kewing/Zotero/storage/6AWCK77D/Williams - 1999 - Method for partitioning a block of data into subbl.pdf:application/pdf},
}

@article{bonwick_zettabyte_2003,
	title = {The {Zettabyte} {File} {System}},
	abstract = {In this paper we describe a new ﬁle system that provides strong data integrity guarantees, simple administration, and immense capacity. We show that with a few changes to the traditional high-level ﬁle system architecture — including a redesign of the interface between the ﬁle system and volume manager, pooled storage, a transactional copy-onwrite model, and self-validating checksums — we can eliminate many drawbacks of traditional ﬁle systems. We describe a general-purpose productionquality ﬁle system based on our new architecture, the Zettabyte File System (ZFS). Our new architecture reduces implementation complexity, allows new performance improvements, and provides several useful new features almost as a side eﬀect.},
	language = {en},
	author = {Bonwick, Jeﬀ and Ahrens, Matt and Henson, Val and Maybee, Mark and Shellenbaum, Mark},
	year = {2003},
	file = {Bonwick et al. - The Zettabyte File System.pdf:/Users/kewing/Zotero/storage/VYDLHV7E/Bonwick et al. - The Zettabyte File System.pdf:application/pdf},
}

@book{kim_data_2017,
	address = {Cham},
	title = {Data {Deduplication} for {Data} {Optimization} for {Storage} and {Network} {Systems}},
	isbn = {978-3-319-42278-7 978-3-319-42280-0},
	url = {http://link.springer.com/10.1007/978-3-319-42280-0},
	language = {en},
	urldate = {2023-01-25},
	publisher = {Springer International Publishing},
	author = {Kim, Daehee and Song, Sejun and Choi, Baek-Young},
	year = {2017},
	doi = {10.1007/978-3-319-42280-0},
	keywords = {Data deduplication techniques, De-duplication, Deduplication and cloud-based storage, Deduplication and data storage, Deduplication and network speed, Deduplication and network storage, Duplicate data, Network and storage services},
	file = {Full Text PDF:/Users/kewing/Zotero/storage/Y3NSC3XU/Kim et al. - 2017 - Data Deduplication for Data Optimization for Stora.pdf:application/pdf},
}

@inproceedings{he_data_2010,
	title = {Data deduplication techniques},
	volume = {1},
	doi = {10.1109/FITME.2010.5656539},
	booktitle = {2010 {International} {Conference} on {Future} {Information} {Technology} and {Management} {Engineering}},
	author = {He, Qinlu and Li, Zhanhuai and Zhang, Xiao},
	year = {2010},
	pages = {430--433},
}

@book{mckusick_design_2014,
	title = {The {Design} and {Implementation} of the {FreeBSD} {Operating} {System}},
	isbn = {978-0-321-96897-5},
	abstract = {The most complete, authoritative technical guide to the FreeBSD kernel''s internal structure has now been extensively updated to cover all major improvements between Versions 5 and 11. Approximately one-third of this edition''s content is completely new, and another one-third has been extensively rewritten.   Three long-time FreeBSD project leaders begin with a concise overview of the FreeBSD kernel''s current design and implementation. Next, they cover the FreeBSD kernel from the system-call level down-from the interface to the kernel to the hardware. Explaining key design decisions, they detail the concepts, data structures, and algorithms used in implementing each significant system facility, including process management, security, virtual memory, the I/O system, filesystems, socket IPC, and networking.   This Second Edition   * Explains highly scalable and lightweight virtualization using FreeBSD jails, and virtual-machine acceleration with Xen and Virtio device paravirtualization   * Describes new security features such as Capsicum sandboxing and GELI cryptographic disk protection   * Fully covers NFSv4 and Open Solaris ZFS support   * Introduces FreeBSD''s enhanced volume management and new journaled soft updates   * Explains DTrace''s fine-grained process debugging/profiling   * Reflects major improvements to networking, wireless, and USB support   Readers can use this guide as both a working reference and an in-depth study of a leading contemporary, portable, open source operating system. Technical and sales support professionals will discover both FreeBSD''s capabilities and its limitations. Applications developers will learn how to effectively and efficiently interface with it; system administrators will learn how to maintain, tune, and configure it; and systems programmers will learn how to extend, enhance, and interface with it.   Marshall Kirk McKusick writes, consults, and teaches classes on UNIX- and BSD-related subjects. While at the University of California, Berkeley, he implemented the 4.2BSD fast filesystem. He was research computer scientist at the Berkeley Computer Systems Research Group (CSRG), overseeing development and release of 4.3BSD and 4.4BSD. He is a FreeBSD Foundation board member and a long-time FreeBSD committer. Twice president of the Usenix Association, he is also a member of ACM, IEEE, and AAAS.   George V. Neville-Neil hacks, writes, teaches, and consults on security, networking, and operating systems. A FreeBSD Foundation board member, he served on the FreeBSD Core Team for four years. Since 2004, he has written the "Kode Vicious" column for Queue and Communications of the ACM. He is vice chair of ACM''s Practitioner Board and a member of Usenix Association, ACM, IEEE, and AAAS.   Robert N.M. Watson is a University Lecturer in systems, security, and architecture in the Security Research Group at the University of Cambridge Computer Laboratory. He supervises advanced research in computer architecture, compilers, program analysis, operating systems, networking, and security. A FreeBSD Foundation board member, he served on the Core Team for ten years and has been a committer for fifteen years. He is a member of Usenix Association and ACM.},
	language = {en},
	publisher = {Pearson Education},
	author = {McKusick, Marshall Kirk and Neville-Neil, George V. and Watson, Robert N. M.},
	month = aug,
	year = {2014},
	note = {Google-Books-ID: aY1pBAAAQBAJ},
	keywords = {Computers / Operating Systems / General, Computers / Operating Systems / UNIX},
}

@article{wilson_new_2008,
	title = {The {New} and {Improved} \{{FileBench}\}},
	author = {Wilson, Andrew},
	year = {2008},
}

@inproceedings{heger_workload_2009,
	title = {Workload {Dependent} {Performance} {Evaluation} of the {Btrfs} and {ZFS} {Filesystems}.},
	booktitle = {Int. {CMG} {Conference}},
	author = {Heger, Dominique A},
	year = {2009},
}

@misc{reinsel_data_2017,
	title = {Data {Age} 2025: {The} {Evolution} of {Data} to {Life}-{Critical}},
	publisher = {IDC},
	author = {Reinsel, David and Gantz, John and Rydning, John},
	month = apr,
	year = {2017},
}

@article{xia_comprehensive_2016,
	title = {A {Comprehensive} {Study} of the {Past}, {Present}, and {Future} of {Data} {Deduplication}},
	volume = {104},
	doi = {10.1109/JPROC.2016.2571298},
	number = {9},
	journal = {Proceedings of the IEEE},
	author = {Xia, Wen and Jiang, Hong and Feng, Dan and Douglis, Fred and Shilane, Philip and Hua, Yu and Fu, Min and Zhang, Yucheng and Zhou, Yukun},
	year = {2016},
	pages = {1681--1710},
}
